# Uncertainty as a Defense for Adversarial Attacks in Computer Vision

This repository focuses on utilizing Minimum Prediction Deviation (MPD) as an uncertainty metric to defend against adversarial attacks in computer vision.

## Overview

Computer vision has become a pervasive tool, but its susceptibility to adversarial attacks poses a significant challenge. Adversarial examples, small input changes designed to deceive machine learning models, can lead to incorrect predictions or decisions, known as evasion attacks. In this project, we evaluate the effectiveness of Minimum Prediction Deviation (MPD) as an uncertainty defense in the field of computer vision.

## Dependencies

Make sure you have the following dependencies installed:

- [PyTorch](https://pytorch.org/)
- [NumPy](https://numpy.org/)
- [ART (Adversarial Robustness Toolbox)](https://github.com/Trusted-AI/adversarial-robustness-toolbox)
- [Scikit-learn](https://scikit-learn.org/stable/)
  
You can install them using:

```bash
pip install torch numpy adversarial-robustness-toolbox scikit-learn
```
## Dataset

### MNIST

- The MNIST database of handwritten digits has a training set of 60,000 examples and a test set of 10,000 examples.
- Each image is a 28x28 pixel grayscale representation of a handwritten digit (0 through 9).
- This dataset is widely used for training and evaluating machine learning models in image recognition, providing a diverse range of writing styles and digit variations.

## Adversarial Robustness Toolbox (ART)

- Adversarial Robustness Toolbox (ART) is a Python library for Machine Learning Security.
- ART provides tools to evaluate, defend, certify, and verify machine learning models and applications against adversarial threats.
- It supports all popular machine learning frameworks (TensorFlow, Keras, PyTorch, MXNet, scikit-learn, XGBoost, LightGBM, CatBoost, GPy, etc.), all data types (images, tables, audio, video, etc.), and machine learning tasks (classification, object detection, generation, certification, etc.).

## Adversarial Attacks

Different adversarial attacks used in the code:

- Basic Iterative Method
- Projected Gradient Descent
- Auto Projected Gradient Descent
- Carlini and Wagner Attack
- Adversarial Patch

## Minimum Prediction Deviation (MPD)

MPD is a metric that measures the uncertainty of a machine learning model's prediction for a single sample. It relies on the distribution of probabilistic predictions generated by an ensemble of bootstrapped estimators. MPD quantifies inconsistencies within the model's predictions in a meaningful manner.

## Flow of the Code

1. **Import the Dataset:**
   - Load the MNIST dataset, consisting of a training set of 60,000 examples and a test set of 10,000 examples, each being a 28x28 pixel grayscale representation of a handwritten digit (0 through 9).

2. **Create a Neural Net Model:**
   - Create a neural network model for image classification using PyTorch.

3. **Create an ART Classifier:**
   - Create an ART classifier and pass the PyTorch model to it.

4. **Train on the Training Set:**
   - Train the neural network model on the training set.

5. **Evaluate on the Test Set:**
   - Evaluate the model's accuracy on the test set.

6. **Generate Adversarial Examples:**
   - Use various adversarial attacks (e.g., Basic Iterative Method, Projected Gradient Descent, Auto PGD, Carlini and Wagner Attack, Adversarial Patch) to generate adversarial examples for the entire test set.

7. **Employ MPD for Detection:**
   - Utilize Minimum Prediction Deviation (MPD) as an uncertainty metric to detect adversarial examples generated by the attacks.

